% !TEX root = ../paper.tex

\section{Evaluation}

This section describes the result of the empiric evaluation of both described algorithms as well as a comparison with a newer algorithm by \citeauthor{braginsky_cbpq:_2016}.

\subsection{Single-Writer-Multi-Reader Lock Implementation}

For conducting the experiments a Sun SPARC T5240 was used, which consists of two UltraSPARC T2 Plus chips with 8 cores each, running at 1.165~GHz. Each core provides 8 hardware threads, for a total of 128 threads (64 per chip). Each run was repeated 5 times. The median was used in the charts. Unfortunately the variance was not quantified in the paper. The throughput was measured in a timespan of 10 seconds. To get stable results the priority queue was seeded with 2000 initial elements. The decision whether to perform an \texttt{add()}-operation or an \texttt{removeMin()}-operation was done randomly with a probability of $p$ resulting in one operation and $1-p$ in the other operation.

The implementation (\textit{pqe}) was compared with 4 other priority queue implementations:
\begin{itemize}
	\item The flat combining skiplist (\textit{fcskiplist}) and the flat combining pairing heap (\textit{fcpairheap}) implementation are both based on the work of \citeauthor{hendler_flat_2010} and use coarse grained locking on a sequential datastructure with flat combining to implement a priority queue algorithm \cite{hendler_flat_2010}.
	\item The lock free skiplist (\textit{lfskiplist}) by \citeauthor{sundell_fast_2003} is based on a parallel skiplist and uses no atomic operations to achieve its lock-freeness. \cite{sundell_fast_2003}.
	\item The Lazy skiplist (\textit{lazyskiplist}) by \citeauthor{lotan_skiplist-based_2000} is based on a parallel skiplist which uses a min-pointer and delete-flags to mark items as taken \cite{lotan_skiplist-based_2000}.
\end{itemize}

The evaluation used different scenarios for evaluating the algorithms by varying the proportion of \texttt{add()} operations and \texttt{removeMin()} operations conversely. In figure~\ref{fig:sparc_50} the results for 50~\% \texttt{add()} and 50~\% \texttt{removeMin()} are shown. The use of elimination suggests a good performance in this scenario. As seen in the diagram, the algorithm \textit{pqe} indeed performs very well. There is a steady increase in throughput up to the maximum thread-count ($60$) tested, but as the throughput seems to level out around 60 threads it can be assumed that the maximum throughput is reached around this number of threads, especially as the machine had to use the second processor for more than 64 threads. The other algorithms reach their peek-throughput between 4 and 8 threads. For low number of threads ($\le 8$) the algorithm is significantly outperformed by \textit{fcpairheap} because of the overhead for elimination and combining in \textit{pqe}.

\begin{figure}[htb]
	\centering
	\begin{minipage}[b]{.495\textwidth}
		\centering
		\includegraphics[width=\linewidth]{graphics/sparc-50-50.pdf}
		\caption{Priority queue performance with 50\% \texttt{add()}s, 50\% \texttt{removeMin()}s \cite{calciu_adaptive_2014}.}
		\label{fig:sparc_50}
	\end{minipage}
	\hfill%
	\begin{minipage}[b]{.495\textwidth}
		\centering
		\includegraphics[width=\linewidth]{graphics/sparc-80-20.pdf}
		\caption{Priority queue performance with 80\% \texttt{add()}s, 20\% \texttt{removeMin()}s \cite{calciu_adaptive_2014}.}
		\label{fig:sparc_80}
	\end{minipage}
	\begin{minipage}[b]{.495\textwidth}
		\centering
		\includegraphics[width=\linewidth]{graphics/sparc-20-80.pdf}
		\caption{Priority queue performance with 20\% \texttt{add()}s, 80\% \texttt{removeMin()}s \cite{calciu_adaptive_2014}.}
		\label{fig:sparc_20}
	\end{minipage}
\end{figure}

Figure~\ref{fig:sparc_80} shows the results for 80~\% \texttt{add()} and 20~\% \texttt{removeMin()}. The use of parallel \texttt{add()} operations still suggests a good performance in this scenario although elimination can't be used that often. Up until 8 threads the \textit{pqe} scales very well with a steady increase in throughput, but from 16 threads on the throughput drops again. The \textit{lazyskiplist} has a similar performance curve, with lower throughput for $\le 16$ threads but a similar decrease from 16 threads on. The curve of \textit{fcpairheap} with an almost static throughput over all tested threadcounts suggests a higher performance than \textit{pqe} and \textit{lazyskiplist} for higher threadcounts than the ones tested. The throughput of about zero for \textit{fcskiplist} seems odd.

The results of the third scenario were not published in the paper by \citeauthor{calciu_adaptive_2014} but the sourcecode located on the \citefield{calciu_adaptive_2014-1}{journaltitle} website contained the results shown in figure~\ref{fig:sparc_20}. The design of the priority queue with sequential \texttt{removeMin()} operations suggests no good performance in this scenario. As seen in the diagram the \textit{pqe} scales quite good with increasing threadcount but has very little throughput in comparison with the other implementations. Other implementations show increasing throughput up to 8 threads, or 16 for \textit{fcpairheap}, but decrease in throughput for higher threadcounts.  Running with $\le 8$ threads \textit{pqe} shows the worst performance of the compared priority queue implementations. \textit{Fcpairheap} outperforms \textit{pqe} by 50~\% to 400~\%. The reason for increasing throughput between 32 and 48 threads is very unexpected and would need further investigation \cite{calciu_adaptive_2014-1}.

In conclusion the \textit{pqe} implementation performs well in scenarios with a balanced operation-proportion and in scenarios with more \texttt{add()} operations but has significant performance issues in scenarios with a high \texttt{removeMin()} operation-proportion. This evaluation shows that the implementation by \citeauthor{calciu_adaptive_2014} is no general purpose parallel priority queue and developers still have to evaluate their usecase to find the best performing priority queue implementation.

\subsubsection{Operation Analysis}

Figure~\ref{fig:sparc_add} shows a diagram breaking down the \texttt{add()} operations optimizations in the different scenarios. While in the first scenario with 80~\% \texttt{add()} operations, parallel \texttt{add()} operations account for more than 75~\% of all the \texttt{add()} operations, they only account for about 5~\% in the balanced scenario and none in the third scenario. Operations executed by the server play only a little role as the account for less than 5~\% in the first and second scenario and none in the third. The remaining fraction is executed via elimination.

\begin{figure}[htb]
	\centering
	\begin{minipage}[b]{.495\textwidth}
		\centering
		\includegraphics[width=\linewidth]{graphics/sparc-add-brk.pdf}
		\caption{\texttt{add()} work breakdown \cite{calciu_adaptive_2014}.}
		\label{fig:sparc_add}
	\end{minipage}
	\hfill%
	\begin{minipage}[b]{.495\textwidth}
		\centering
		\includegraphics[width=\linewidth]{graphics/sparc-rem-brk.pdf}
		\caption{\texttt{removeMin()} work breakdown \cite{calciu_adaptive_2014}.}
		\label{fig:sparc_rem}
	\end{minipage}
\end{figure}

The \texttt{removeMin()} operations optimizations are shown in figure~\ref{fig:sparc_rem}. In the first scenario with 20~\% \texttt{removeMin()} operations, elimination accounts for about 75~\% of all the operations executed. In the balanced scenario elimination accounts for about 90~\% of all the executions, while in the last case it is only about 25~\%. This explains the low throughput in this scenario, as 75~\% of all the \texttt{removeMin()} operations, which make up 60~\% of all the operations executed, are executed via the server thread.

\subsection{Hardware Transactional Memory Implementation}

To evaluate the performance of the hardware transactional memory implementation a Intel Core i7-4770 based computer was used. The four cores run at 3.4~GHz and provide two hardware threads each. The processor had hardware transactions enabled  using restricted transactional memory (RTM) and used all 8 possible hardware threads with hyperthreading. Hyperthreading can influence the performance negatively because of the cache being shared between the 2 running hyperthreads on a core. The application was compiled using the GCC~4.8 compiler with support for RTM and optimizations enabled (-O3).

Figure~\ref{fig:tsx1} shows the evaluation results in a scenario with 80~\% \texttt{add()} and 20\%\texttt{removeMin()} operations for the lock-based implementation (\textit{pqe}) and the hardware-transaction based implementation (\textit{pqe-tsx}). As seen in the diagram the transaction base implementation starts off with the same throughput for $1+1$ threads. The throughput-increase is higher than that for \textit{pqe} resulting in a 20\% higher throughput for $1+2$ threads and a 40\% higher throughput for $1+7$ threads.

The second scenario tested with equally many \texttt{add()} and \texttt{removeMin()} operations is shown in figure~\ref{fig:tsx2}. The results obtained in the experiments show almost no difference between the throuput of both implementations. This difference to the other scenario can be explained with the work breakdown shown in figure~\ref{fig:sparc_add}. The use of hardware transactional memory speeds up the parallel \texttt{add()} operations, not affecting the performance of other operations in the priority queue. As the first scenario uses about 75\% parallel \texttt{add()} operations (60\% of all operations) and the second uses only about 5\% (2.5\% of all operations), the throughput is hardly affected. 
\begin{figure}[htb]
	\centering
	\begin{minipage}{.495\textwidth}
		\centering
		\includegraphics[width=\linewidth]{graphics/tsx-80-20.pdf}
		\caption{Priority queue performance using a transaction-based skiplist with 80\% \texttt{add()} and 20\% \texttt{removeMin()} operations \cite{calciu_adaptive_2014}.}
		\label{fig:tsx1}
	\end{minipage}%
	\hfill%
	\begin{minipage}{.495\textwidth}
		\centering
		\includegraphics[width=\linewidth]{graphics/tsx-50-50.pdf}
		\caption{Priority queue performance using a transaction-based skiplist with 50\% \texttt{add()}s and 50\% \texttt{removeMin()} operations \cite{calciu_adaptive_2014}.}
		\label{fig:tsx2}
	\end{minipage}
\end{figure}

\subsection{Comparison with Algorithm by \citeauthor{braginsky_cbpq:_2016}}

\citeauthor{braginsky_cbpq:_2016} claim in their paper that they implemented a high performing, lock free, parallel priority queue. They used the original sourcecode by \citeauthor{calciu_adaptive_2014} for conducting their experiments. The evaluation environment used differs from the one used by \citeauthor{calciu_adaptive_2014}, as they used a machine with 4 16-core AMD Opteron (TM) 6272 processors having a total of 64 threads. The algorithms were all written in C++ and compiled with a -O3 optimization level on Ubuntu~14.04 \cite{braginsky_cbpq:_2016}.

\newpage
Figures~\ref{fig:cbpq_50}, \ref{fig:cbpq_80} and \ref{fig:cbpq_20} show the throughput of the conducted experiments to compare the \textit{CBPQ} algorithm by \citeauthor{braginsky_cbpq:_2016} to other high performing concurrent priority queue implementations at the time:
\begin{itemize}
	\item The lock free priority queue (\textit{LJPQ}) by \citeauthor{linden_skiplist-based_2013} is  based on a skiplist and tries to reduce global memory updates to decrease contention in \texttt{removeMin()} operations \cite{linden_skiplist-based_2013}.
	\item The lock free mound (\textit{Mound LF}) and fine grained locking mound (\textit{Mound LB}) by \citeauthor{liu_mounds:_2012} based on the mound datastructure. A mound is a randomized balanced tree
	of sorted lists that allows concurrent \texttt{insert()} and \texttt{extractMin()} operations \cite{liu_mounds:_2012}.
	\item The priority queue (\textit{APQ}) this paper is based on by \citeauthor{calciu_adaptive_2014} \cite{calciu_adaptive_2014}.
\end{itemize}

\begin{figure}[htb]
	\centering
	\begin{minipage}[b]{.7\textwidth}
		\centering
		\includegraphics[width=\linewidth]{graphics/cbpq_50add_50.png}
		\caption{Priority queue throughput in million instructions per second with 50\% \texttt{add()}s, 50\% \texttt{removeMin()}s \cite{braginsky_cbpq:_2016}.}
		\label{fig:cbpq_50}
	\end{minipage}
	\hfill%
	\begin{minipage}[b]{.495\textwidth}
		\centering
		\includegraphics[width=\linewidth]{graphics/cbpq_80add_20.png}
		\caption{Priority queue throughput in million instructions per second with 80\% \texttt{add()}s, 20\% \texttt{removeMin()}s \cite{braginsky_cbpq:_2016}.}
		\label{fig:cbpq_80}
	\end{minipage}
	\begin{minipage}[b]{.495\textwidth}
		\centering
		\includegraphics[width=\linewidth]{graphics/cbpq_20add_80.png}
		\caption{Priority queue throughput in million instructions per second with 20\% \texttt{add()}s, 80\% \texttt{removeMin()}s \cite{braginsky_cbpq:_2016}.}
		\label{fig:cbpq_20}
	\end{minipage}
\end{figure}

The scalability of \textit{APQ} is much worse on the AMD platform than on the original Sun platform. Figure~\ref{fig:cbpq_50} shows that in the scenario with 50\% \texttt{add()} operations the algorithm by \citeauthor{calciu_adaptive_2014} scales well until 7 threads and drops significantly afterwards while other implementations scale better or at least achieve much higher throughput for the tested threadcounts. It remains to mention a good performance of \textit{APQ} for low threadcounts ($\le 7$) in comparison with the test's lock free priority queues except \textit{LJPQ}.

The scenario with 80\% \texttt{add()} operations in figure~\ref{fig:cbpq_80} shows a very similar curve than the diagram in figure~\ref{fig:sparc_80}. \textit{APQ} is outperformed by every other implementation for $\le 15$ threads in this scenario, only the lock free mound implementation (\textit{Mound LF}) drops below \textit{APQ} for higher threadcounts.

Figure~\ref{fig:cbpq_20} again shows a poor performance of APQ in comparison with other implementations. The throughput development over the threadcount on this platform is quite different from the one shown in figure~\ref{fig:sparc_20}. The throughput increases until 5 threads but drops afterwards. It can be seen that on the AMD system there is no mysterious performance increase between 32 and 48 threads, as the throughput drops continuously. 

\subsection{Evaluation Remarks}

While reading the original paper, a few odd things came up. The Sun SPARC T5240 machine used for conducting the experiments was introduced in 2008 while the paper was published in 2014. The Intel CPU they used was a current model though. Using such an old machine in spite of obviously having a current machine makes you wonder if it was used because the algorithm shows better results on this old machine. Additionally, the experiment was only conducted for up to 60 threads although the machine supports up to 128 threads. The experiment therefore never utilized the second processor.

It would be interesting to know how the priorities used while adding elements were determined. The operating system used was never mentioned. A uniform scaling of the different diagrams' axis would increase readability and would ease the comparison between the different diagrams. Especially the scaling of the x-axis makes it hard to analyze the changes between two succeeding values.





