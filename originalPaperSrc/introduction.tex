\section{Introduction}
\label{Sec-Introduction}

A priority queue is a fundamental abstract data structure that stores a set of keys (or a set of key-value pairs), where keys represent priorities. It usually exports two main operations: \texttt{add()}, to insert a new item in the priority queue, and \texttt{removeMin()}, to remove the first item (the one with the highest priority). Parallel priority queues are often used in discrete event simulations and resource management, such as operating systems schedulers. Therefore, it is important to carefully design these data structures in order to limit contention and improve scalability. Prior work in concurrent priority queues exploited parallelism by using either a heap~\cite{pqhunt} or a skiplist~\cite{Lotan2000} as the underlying data structures. In the skiplist-based implementation of Lotan and Shavit~\cite{Lotan2000} each node has a ``deleted'' flag, and processors contend to mark such ``deleted'' flags concurrently, in the beginning of the list. When a thread logically deletes a node, it tries to remove it from the skiplist using the standard removal algorithm. A lock-free skiplist implementation is presented in \cite{pqsundelltsigas}. 

However, these methods may incur limited scalability at high thread counts due to contention on shared memory accesses. Hendler et al.~\cite{Hendler2010} introduced Flat Combining, a method for batching together multiple operations to be performed by only one thread, thus reducing the contention on the data structure. This idea has also been explored in subsequent work on delegation~\cite{Metreveli2012,CalciuDHHKMM13}, where a dedicated thread called a \emph{server} performs work on behalf of other threads, called \emph{clients}. Unfortunately, the server thread could become a sequential bottleneck. A method of combining delegation with elimination has been proposed to alleviate this problem for a stack data structure~\cite{HotPar13Stack}. Elimination~\cite{Hendler2010a} is a method of matching concurrent inverse operations so that they don't access the shared data structure, thus significantly reducing contention and increasing parallelism for otherwise sequential structures, such as stacks. An elimination algorithm has also been proposed in the context of a queue~\cite{Moir2005}, where the authors introduce the notion of \emph{aging operations} - operations that wait until they become suitable for elimination.


In this paper, we describe, to the best of our knowledge, the first elimination algorithm for a priority queue.
Only \texttt{add()} operations with values smaller than the priority queue minimum value are allowed to eliminate. However, we use the idea of aging operations introduced in the queue algorithm to allow \texttt{add()} values that are \emph{small enough} to participate in the elimination protocol, in the hope that they will soon become eligible for elimination.
We implement the priority queue using a skiplist and we exploit the skiplist's capability for both operations-batching and disjoint-access parallelism. 
\texttt{RemoveMin()} requests can be batched and executed by a server thread using the combining/delegation paradigm. 
 \texttt{Add()} requests with high keys will most likely not become eligible for elimination and need to be inserted in the skiplist, sometimes requiring expensive traversals towards the end of the data structure to do so. Therefore, these operations represent a bottleneck for the server and a missed opportunity for parallelism. To alleviate these issues, we split the underlying skiplist into two parts: a \emph{sequential} part, managed by the server thread and a \emph{parallel} part, where high-valued \texttt{add()} operations can insert their arguments in parallel. Our design reduces contention by performing batched sequential \texttt{removeMin()} and small-value \texttt{add()} operations, while also leveraging parallelism opportunities through elimination and parallel high-value \texttt{add()} operations. We show that our priority queue outperforms prior algorithms in high contention workloads on a SPARC Niagara II machine. Finally, we explore whether the use of hardware transactions could simplify our design and improve throughput. Unfortunately, machines that support hardware transactional memory (HTM) are only available for up to four cores (eight hardware threads), which is not enough to measure scalability of our design in high contention scenarios. Nevertheless, we showed that a transactional version of our algorithm is better than a non-transactional version on a Haswell four-core machine. We believe that these preliminary results will generalize to machines with more threads with support for HTM, once they become available. In summary, our main contributions are:
\begin{itemize}
\item We propose the first elimination algorithm for a priority queue, consisting of (1)~\emph{immediate elimination}, where suitable \texttt{add()} and \texttt{removeMin()} operations exchange arguments; and (2)~\emph{upcoming elimination}, where \texttt{add()} operations with small keys, yet not suitable for elimination, wait some time until either they become suitable or time out.
\item We describe a scalable design for a priority queue based on our elimination algorithm and the delegation/combining paradigm introduced by prior work.
\item We augment our priority queue design with an adaptive component that allows it to perform combining and elimination efficiently, while also allowing \texttt{add()} operations not involved in the elimination to insert in parallel. 
\item We analyze how hardware transactions could be used to simplify and improve our initial design and show performance results on a Haswell machine with transactional memory enabled. 
\end{itemize}
